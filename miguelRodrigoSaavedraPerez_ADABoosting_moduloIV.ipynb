{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting (ADA)\n",
    "\n",
    "Mostraremos como el AdaBoost.M1 es equivalente al modelado aditivo por etapas usando la función de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solución para la ecuación anterior se puede obtener por dos pasos.\n",
    "\n",
    "Uno. Para cualquier valor $\\beta$ > 0 la solución para $G_{m}(x)$ donde\n",
    "\n",
    "entonces los pesos estan definidos por $w_{i}^{(m)} = e^{-y_{i}f_{(m-1)}(x_{i})}$, y dividimos los datos en dos subconjuntos: ${y_{i} = G(x_{i})}$ and ${y_{i} \\neq G(x_{i})}$,\n",
    "\n",
    "$\\hat{G}_{m} = arg min_{G} \\sum_{i=1}^{N}w_{i}^{m} I(y_{i} \\neq G(x_{i}))$\n",
    "\n",
    "$e^{-\\beta} \\sum_{y_{i}=G(x_{i})} w_{i}^{m} + e^{\\beta} \\sum_{y_{i} \\neq G(x_{i})} w_{i}^{m}$\n",
    "\n",
    "$(e^{\\beta} - e^{-\\beta}) \\sum_{i=1}^{N}w_{i}^{m} I(y_{i} \\neq G(x_{i})) + e^{-\\beta} \\sum_{i=1}^{N}w_{i}^{m}$\n",
    "\n",
    "Usamos $G_{m}$ para resolver $(\\beta_{m},G_{m})  = arg min _{\\beta,G} \\sum_{i=1}^{N}w_{i}^{m}e^{(−\\beta y_{i} G(x_{i}))}$ y solucionar para $\\beta$ obtenemos:\n",
    "\n",
    "Obtenemos $\\beta$ aplicamos logaritmos y derivamos para obtener el $\\beta$ con el que se hace mínimo:\n",
    "\n",
    "$log(e^{\\beta}err_{m}+e^{-\\beta}(1-err_{m})) →$ derivamos respecto a $\\beta$ e igualamos a 0\n",
    "\n",
    "$→ e^{\\beta}err_{m} - e^{-\\beta}(1-err_{m}) = 0$\n",
    "\n",
    "$→ \\beta + log(err_{m}) = -\\beta + log(1-err_{m})$\n",
    "\n",
    "$→ 2\\beta = log(1-err_{m}) - log(err_{m})$\n",
    "\n",
    "$→ \\beta_{m} = \\frac{1}{2} log \\frac{1-err_{m}} {err_{m}}$\n",
    "\n",
    "Donde\n",
    "\n",
    "$err_{m} = \\frac{\\sum_{i=1}^{N}w_{i}^{m} I(y_{i} \\neq G(x_{i})) }{\\sum_{i=1}^{N}w_{i}^{m}}$\n",
    "\n",
    "La aproximación entonces se actualiza a:\n",
    "\n",
    "$f_{(m)}(x) = f_{(m-1)}(x) + \\beta_{m}G_{m}(x)$\n",
    "\n",
    "Por tanto las interacciones para la siguiente iteración es:\n",
    "\n",
    "$w_{i}^{(m+1)}= w_{i}^{m} e^{-\\beta_{m} y_{i} G_{m} (x_{i})}$\n",
    "\n",
    "Usando que $-y_{i}G_{m}(x{i}) = 2 * I(y{i} \\neq G_{m}(x{i}))-1$ llegamos a\n",
    "\n",
    "$w_{i}^{(m+1)}=w_{i}^{(m)} * e^{2\\beta_{m} I(y{i} \\neq G_{m}(x{i}))} * e^{-\\beta{m}}$\n",
    "\n",
    "Ahora resumiendo el algoritmo tenemos:\n",
    "\n",
    "1. Inicializamos los pesos $w_{i} = \\frac{1}{N}, i = 1, . . . , N$  \n",
    "\n",
    "2. Para m = 1 a M\n",
    "\n",
    "2.1 Ajustamos el clasificador $G_{m}$ para entrenar los datos con pesos $w_{i}$\n",
    "\n",
    "2.2 Actualizamos los pesos por $w_{i} ← w_{i} e^{\\alpha_{m} I(y_{i} \\neq G_{m}(x_{i}))}$, donde $\\alpha_{m} = log \\frac{1 − err_{m}}{err_{m}}$ y $err_{m} = \\frac{\\sum_{i=1}^{N}w_{i}^{(m)} I(y_{i} \\neq G(x_{i})) }{\\sum_{i=1}^{N}w_{i}^{(m)}}$\n",
    "\n",
    "3. Salida final del clasificador $f(x) = sign( \\sum_{m=1}^{M} \\alpha_{m} G_{m}(x))$\n",
    "\n",
    "Que es exactamente AdaBoost.\u0011\n",
    "\n",
    "El factor $e^{\\beta_{m}}$ multiplica a todos los pesos por el mismo valor, por tanto, no es necesario y\n",
    "se puede eliminar. Así, obtenemos una expresión equivalente a la que dábamos cuando\n",
    "explicamos el algoritmo AdaBoost.M1. Con esto, podemos concluir que AdaBoost.M1\n",
    "minimiza una función de pérdida exponencial mediante un modelo aditivo por pasos hacia\n",
    "adelante.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
